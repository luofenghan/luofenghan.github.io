{"meta":{"title":"心之所至、意之使然","subtitle":null,"description":null,"author":"落枫寒","url":"http://www.vibrancy.cn"},"pages":[{"title":"关于","date":"2017-01-16T15:24:04.000Z","updated":"2017-01-16T15:24:29.826Z","comments":true,"path":"about/index.html","permalink":"http://www.vibrancy.cn/about/index.html","excerpt":"","text":""},{"title":"文章","date":"2017-01-16T15:31:12.000Z","updated":"2017-04-29T02:19:33.383Z","comments":true,"path":"archives/index.html","permalink":"http://www.vibrancy.cn/archives/index.html","excerpt":"","text":""},{"title":"分类","date":"2017-01-16T15:21:44.000Z","updated":"2017-01-16T15:22:19.052Z","comments":true,"path":"categories/index.html","permalink":"http://www.vibrancy.cn/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-01-16T15:22:46.000Z","updated":"2017-01-16T15:23:17.675Z","comments":true,"path":"tags/index.html","permalink":"http://www.vibrancy.cn/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"交换排序","slug":"交换排序","date":"2017-05-11T04:45:27.000Z","updated":"2017-05-11T04:54:58.723Z","comments":true,"path":"2017/05/11/交换排序/","link":"","permalink":"http://www.vibrancy.cn/2017/05/11/交换排序/","excerpt":"","text":"冒泡排序排序思想通过交换使相邻的两个数变成小数在前大数在后，这样每次遍历后，最大的数就“沉”到最后面了，重复N次即可以使数组有序。 算法实现 动图演示 排序过程示意图 代码实现 123456789101112public static class BubbleSort &#123; public static void sort(int[] array) &#123; for (int i = 0; i &lt; array.length; i++) &#123; // j &lt; array.length - i - 1 意思是后面的已经有序，不需要在判断 for (int j = 0; j &lt; array.length - i - 1; j++) &#123; if (array[j] &gt; array[j + 1]) &#123; Utils.swap(array, j, j + 1); &#125; &#125; &#125; &#125;&#125; 结论冒泡排序是基于比较的算法，时间复杂度为\\(O(N^2)\\)，只有在n比较小的时候性能才比较好。 算法改进 设置一个标志性变量pos，用于记录每趟排序中最后一次进行交换的位置。由于pos之后的记录均已交换到位，因此在下一趟排序时只要扫描到pos位置即可。 123456789101112public static void sort1(int[] array) &#123; for (int i = array.length - 1; i &gt; 0; ) &#123; int pos = 0; for (int j = 0; j &lt; i; j++) &#123; if (array[j] &gt; array[j + 1]) &#123; pos = j; Utils.swap(array, j, j + 1); &#125; &#125; i = pos; &#125;&#125; 传统冒泡排序在每趟的操作中只能找到一个最大值或最小值，因此，考虑利用在每趟排序中进行正向和反向的两边冒泡方法一次可以得到两个最终值（最大值和最小值），从而使排序趟数几乎减少一半。 12345678910111213141516171819public static void sort2(int[] array) &#123; int low = 0; int high = array.length - 1; while (low &lt; high) &#123; for (int i = low; i &lt; high; i++) &#123; if (array[i] &gt; array[i + 1]) &#123; Utils.swap(array, i, i + 1); &#125; &#125; high--; for (int i = high; i &gt; low; i--) &#123; if (array[i] &lt; array[i - 1]) &#123; Utils.swap(array, i, i - 1); &#125; &#125; low++; &#125;&#125; 快速排序排序思想 选取一个基准pivot元素，通常选择第一个元素或者最后一个元素； 进行分区partition操作，通过一趟排序将待排序的记录分割成两个部分，其中一个部分的元素均比基准元素小，另一部分元素均比基准元素大； 对每个分区递归地进行步骤1~3，递归的结束条件是子序列的大小是0或1，这时整体已经排好序。 算法实现 动图演示 排序过程 代码实现 12345678910111213141516171819202122232425public static void sort(int[] array) &#123; quickSort(array, 0, array.length - 1);&#125;private static void quickSort(int[] array, int low, int high) &#123; if (low &lt; high) &#123; int pivot = partition(array, low, high); quickSort(array, 0, pivot - 1); quickSort(array, pivot + 1, high); &#125;&#125;private static int partition(int[] array, int low, int high) &#123; for (int pivot = array[low]; low &lt; high; ) &#123; while (low &lt; high &amp;&amp; array[high] &gt;= pivot) &#123; high--; &#125; Utils.swap(array, low, high); while (low &lt; high &amp;&amp; array[low] &lt;= pivot) &#123; low++; &#125; Utils.swap(array, low, high); &#125; return low;&#125; 结论 最坏的情况下，也就是每次选取的基准都是最大或最小的元素（例如，在上例7,8,10,9中），导致每次只划分出了一个子序列，需要进行n-1次划分才能结束递归，时间复杂度为\\(O(n^2)\\)； 最好的情况下，每次选取的基准都能均匀划分，只需要\\(logN\\)次划分就能结束递归，时间复杂度为\\(O(logN)\\)。 平均情况下，需要的时间复杂度为\\(O(NlogN)\\)。 快速排序不是稳定的排序算法。 快速排序的改进快速排序通常被认为在同数量级\\(O(NlogN)\\)的排序方法中性能最好的，若初始序列已经基本有序，快排反而退化为冒泡排序。 在改进的算法中，只对长度大于k的子序列递归调用快速排序，让原序列基本有序，然后再对整个基本有序的序列使用直接插入排序。实践证明，改进后的算法时间复杂度有所降低，且当k取8左右的时候，改进算法的性能最优。 1234567891011121314151617181920212223242526public static void sort(int[] array, int k) &#123; quickSortImprove(array, 0, array.length - 1, k); InsertSort.StraightInsertSort.sort(array);&#125;private static void quickSortImprove(int[] array, int low, int high, int k) &#123; if (high - low &gt; k) &#123; int pivot = partition(array, low, high); quickSortImprove(array, 0, pivot - 1, k); quickSortImprove(array, pivot + 1, high, k); &#125;&#125;private static int partition(int[] array, int low, int high) &#123; for (int pivot = array[low]; low &lt; high; ) &#123;//从表的两端交替地向中间扫描 while (low &lt; high &amp;&amp; array[high] &gt;= pivot) &#123; //从high 所指位置向前搜索，至多到low+1 位置。将比基准元素小的交换到低端 high--; &#125; Utils.swap(array, low, high); while (low &lt; high &amp;&amp; array[low] &lt;= pivot) &#123; low++; &#125; Utils.swap(array, low, high); &#125; return low;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"http://www.vibrancy.cn/categories/算法/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://www.vibrancy.cn/tags/算法/"},{"name":"交换排序","slug":"交换排序","permalink":"http://www.vibrancy.cn/tags/交换排序/"}]},{"title":"选择排序","slug":"选择排序","date":"2017-05-11T04:44:26.000Z","updated":"2017-05-11T04:56:16.259Z","comments":true,"path":"2017/05/11/选择排序/","link":"","permalink":"http://www.vibrancy.cn/2017/05/11/选择排序/","excerpt":"","text":"简单选择排序排序思想按照索引顺序，每趟会在该索引后的元素中找出一个最小元素与当前索引处的元素进行交换。 算法实现 简单排序过程示例 动图演示 代码实现 12345678910111213141516public static class SimpleSelectSort &#123; public static void sort(int[] array) &#123; for (int i = 0; i &lt; array.length; i++) &#123; int minValue = array[i]; int minIndex = i; for (int j = i + 1; j &lt; array.length; j++) &#123; if (array[j] &lt; minValue) &#123; minValue = array[j]; minIndex = j; &#125; &#125; array[minIndex] = array[i]; array[i] = minValue; &#125; &#125;&#125; 结论 简单选择排序的时间复杂度都为\\(O(N^2)\\)； 是不稳定的排序算法 简单选择排序的改进将每趟循环可以确定两个元素（最大和最小值），从而减少排序所需的循环次数。 改进后对n个数据进行排序，最多只需进行\\([n/2]\\)趟即可1234567891011121314151617public static void sort2(int[] array) &#123; for (int i = 1, min, max, len = array.length; i &lt;= len / 2; i++) &#123; min = max = i; for (int j = i + 1; j &lt;= len - i; j++) &#123; if (array[j] &gt; array[max]) &#123; max = j; continue; &#125; if (array[j] &lt; array[min]) &#123; min = j; &#125; &#125; //该交换操作还可分情况讨论以提高效率 Utils.swap(array, min, i - 1); Utils.swap(array, max, len - i); &#125;&#125; 堆排序排序思想优先队列可以用于以\\(O(NlogN)\\)时间来排序，基于该思想的算法叫做堆排序heapsort。在建立N个元素的二叉堆时，该阶段花费\\(O(N)\\)时间，然后又执行N次deleteMin操作，由于每个deleteMin花费时间\\(O(logN)\\)，因此总运行时间是\\(O(NlogN)\\)。 优先队列的算法主要问题在于，它使用了一个附加数组，因此，存储需求增加一倍。但不会太影响时间问题，附加的时间消耗只有\\(O(N)\\)，只是增加了空间复杂度。 那么对于以上问题，在堆排序中的解决方案是：在每次deleteMin之后，将堆缩小1。因此，堆中的最后一个单元可以用来存放刚刚删除的元素。使用这种策略，在最后一次deleteMin之后，该数组将以递减的顺序包含这些元素。如果想要排成更典型的递增顺序，那么可以在构建堆的时候建立最大堆。 算法描述 构造最大堆（Build_Max_Heap）：若数组下标范围为0~n，考虑到单独一个元素是大根堆，则从下标\\(n/2\\)开始的元素均为大根堆。于是只要从\\(n/2-1\\)开始，向前依次构造大根堆，这样就能保证，构造到某个节点时，它的左右子树都已经是大根堆。 堆排序（HeapSort）：由于堆是用数组模拟的。得到一个大根堆后，数组内部并不是有序的。因此需要将堆化数组有序化。思想是移除根节点，并做最大堆调整的递归运算。第一次将\\(heap[0]\\)与\\(heap[n-1]\\)交换，再对\\(heap[0…n-2]\\)做最大堆调整。第二次将\\(heap[0]\\)与\\(heap[n-2]\\)交换，再对\\(heap[0…n-3]\\)做最大堆调整。重复该操作直至\\(heap[0]\\)和\\(heap[1]\\)交换。由于每次都是将最大的数并入到后面的有序区间，故操作完后整个数组就是有序的了。 最大堆调整（Max_Heapify）：该方法是提供给上述两个过程调用的。目的是将堆的末端子节点作调整，使得子节点永远小于父节点 。 算法实现 动图演示 代码实现 12345678910111213141516171819202122232425262728293031323334public static class HeapSort &#123; public static void sort(int[] array) &#123; for (int i = array.length / 2; i &gt;= 0; i--) &#123; percolateDown(array, i, array.length); &#125; for (int i = array.length - 1; i &gt; 0; i--) &#123; Utils.swap(array, 0, i); percolateDown(array, 0, i); &#125; &#125; private static int leftChild(int i) &#123; return 2 * i + 1; &#125; private static void percolateDown(int[] a, int i, int n) &#123; int child; int tmp; for (tmp = a[i]; leftChild(i) &lt; n; i = child) &#123; child = leftChild(i); //找到i孩子节点中最大的一个 if (child != n - 1 &amp;&amp; a[child] &lt; a[child + 1]) &#123; child++;//i的右孩子 &#125; if (tmp &lt; a[child]) &#123;//如果较大的子结点大于父结点 a[i] = a[child]; // 那么把较大的子结点往上移动，替换它的父结点 &#125; else &#123; break; &#125; &#125; a[i] = tmp; &#125;&#125; 参考资料 经典排序算法总结与实现 白话经典算法系列 排序算法可视化 所谓堆和堆排序 几种经典排序算法","categories":[{"name":"算法","slug":"算法","permalink":"http://www.vibrancy.cn/categories/算法/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://www.vibrancy.cn/tags/算法/"},{"name":"选择排序","slug":"选择排序","permalink":"http://www.vibrancy.cn/tags/选择排序/"}]},{"title":"插入排序","slug":"插入排序","date":"2017-05-11T04:40:59.000Z","updated":"2017-05-11T04:53:52.621Z","comments":true,"path":"2017/05/11/插入排序/","link":"","permalink":"http://www.vibrancy.cn/2017/05/11/插入排序/","excerpt":"","text":"直接插入排序排序思想按照索引顺序，每一步将该索引上的值插入到前面已经有序的一组的值适当位置（通过从当前索引处往前的挨个比较找到的）上，直到全部插入为止。 算法步骤 从第一个元素开始，该元素可以认为已经被排序； 取出下一个X元素，在已经排序的元素序列中从后向前扫描； 如果扫描到的元素（已排序）大于X元素，将该元素往后移动一个位置； 重复步骤3，直到找到已排序的元素小于或者等于X元素的位置； 将X元素插入到该位置； 重复步骤2~4。 算法实现 动图演示 代码实现 123456789101112static class StraightInsertSort &#123; static void sort(int[] e) &#123; int j; for (int i = 1; i &lt; e.length; i++) &#123; int x = e[i]; for (j = i - 1; j &gt;= 0 &amp;&amp; e[j] &gt; x; j--) &#123; e[j + 1] = e[j]; &#125; e[j + 1] = x; &#125; &#125;&#125; 结论 当输入数据以反序输入时，直接插入排序的时间复杂度为\\(O(N^2)\\)，因为由于嵌套循环的每一个都花费N次迭代； 当输入数据已预先排序，直接插入排序的时间复杂度为\\(O(N)\\)，因为内层的for循环的检测总是立即判定不成立而终止。 插入排序适合数据量比较小的排序应用； 逆序数也正好是需要由插入排序执行的交换次数，而一个排过序的数组没有逆序。当输入数据是34,8,64,51,32,21时，该数据有9个逆序，即(34,8),(34,32),(34,21),(64,51),(64,32),(64,21),(51,32),(51,21)以及(32,21)。由于算法还有\\(O(N)\\)量的其他工作，因此插入排序的运行时间是\\(O(I+N)\\)，其中I为原始数组中的逆序数。于是，若逆序数是\\(O(N)\\)，则插入排序以线性时间运行。 N个互异数的数组的平均逆序数是\\(N(N-1)/4\\)； 通过交换相邻元素进行排序的任何算法平均时间复杂度都需要\\(O(N^2)\\)，也就是说，为了使一个排序算法以\\(O(N^2)\\)时间运行，必须执行一些比较，特别是要对相距较远的元素进行交换。一个排序算法通过删除逆序得以向前进行，而为了有效的进行，他必须使每次交换删除不止一个逆序。 二分插入排序排序思想按照索引顺序，每一步将该索引上的值插入到前面已经有序的一组的值适当位置（通过二分查找法找到，可以减少比较次数）上，直到全部插入为止。 算法实现 代码实现 12345678910111213141516171819202122232425public static class BinaryInsertSort &#123; public static void sort(int[] array) &#123; int left, current, mid, right; for (int i = 1; i &lt; array.length; i++) &#123; current = array[i]; left = 0; right = i - 1; while (left &lt;= right) &#123; mid = (left + right) / 2; if (current &gt; array[mid]) &#123; left = mid + 1; &#125; else &#123; right = mid - 1; &#125; &#125; for (int k = i - 1; k &gt;= left; k--) &#123; array[k + 1] = array[k]; &#125; if (left != i) &#123; array[left] = current; &#125; &#125; &#125;&#125; 结论 当N比较大时，二分插入排序的比较次数比直接插入排序的最差情况要好得多，但是比直接插入排序的最好情况要差。 当元素初始序列接近有序时，直接插入排序比二分插入排序的比较次数少。 二分插入排序元素移动次数与直接插入排序相同，依赖于元素的初始序列。 希尔排序排序思想希尔排序通过将比较的全部元素分为几个区域来提升插入排序的性能，这样可以让一个元素可以一次性地朝最终位置前进一大步。然后算法再取越来越小的步长进行排序，算法的最后一步就是普通的插入排序，但到了这一步，数据几乎已经排好序。 如果有一个很小的数据在一个已按升序排好序的数组的末端，如果用复杂度为\\(O(N^2)\\)的排序算法（冒泡排序或直接插入排序），可能会进行n次的比较和交换才能将该数据移至正确的位置。而希尔排序会用较大的步长移动数据，所以小数据只需要进行少数比较和交换即可到正确位置。 算法步骤算法实现 算法图解 代码实现 123456789101112131415public static class ShellSort &#123; public static void sort(int[] e) &#123; for (int gap = e.length / 2; gap &gt; 0; gap /= 2) &#123; int j; for (int i = gap; i &lt; e.length; i++) &#123; int current = e[i]; for (j = i - gap; j &gt;= 0 &amp;&amp; e[j] &gt; current; j -= gap) &#123; e[j + gap] = e[j]; &#125; e[j + gap] = current; &#125; &#125; &#125;&#125; 结论 \\(h_k\\)排序的实质就是，将\\(h_k\\)，\\(h_k+1\\)，···，\\(N-1\\)中的每个位置i，把该位置对应的元素放到\\(i-{h_k}\\)中的正确位置上。 一趟\\(h_k\\)排序的作用就是对\\(h_k\\)个独立的子数组执行一次插入排序。 使用增量序列\\(h_k\\)进行一趟排序后，对于每个i我们都有\\(a[i]&lt;=a[i+h_k]\\)，所有相隔\\(h_k\\)的元素都被排序，此时称文件时\\(h_k\\)排序的。 一个\\(hk\\)排序的文件（然后是\\(h{k-1}\\)排序）会一直保持它的\\(h_k\\)排序性，前面各趟排序的成果不会被后面的排序打乱。 希尔排序不是稳定的排序算法 虽然一次插入排序是稳定的，不会改变相同元素的相对顺序，但在不同的插入排序过程中，相同的元素可能在各自的插入排序中移动，最后其稳定性将会被打乱。 比如序列{ 3, 5, 10, 8, 7, 2, 8, 1, 20, 6 }h=2时分成两个子序列 { 3, 10, 7, 8, 20 } 和 { 5, 8, 2, 1, 6 } ，未排序之前第二个子序列中的8在前面，现在对两个子序列进行插入排序，得到 { 3, 7, 8, 10, 20 } 和 { 1, 2, 5, 6, 8 } ，即 { 3, 1, 7, 2, 8, 5, 10, 6, 20, 8 } ，两个8的相对次序发生了改变。 插入排序综合比较 名称 最差时间复杂度 最优时间复杂度 平均时间复杂度 辅助空间 稳定性 直接插入排序 \\(O(N^2)\\) \\(O(N)\\) \\(O(N^2)\\) \\(O(1)\\) 稳定 二分插入排序 \\(O(N^2)\\) \\(O(NlogN)\\) \\(O(N^2)\\) \\(O(1)\\) 稳定 希尔排序 根据步长序列的不同而不同，最坏是\\(O(N^2)\\) \\(O(N)\\) 根据步长的不同而不同 \\(O(1)\\) 不稳定 参考资料 常用排序算法总结(一) 八大排序，各显神通","categories":[{"name":"算法","slug":"算法","permalink":"http://www.vibrancy.cn/categories/算法/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://www.vibrancy.cn/tags/算法/"},{"name":"插入排序","slug":"插入排序","permalink":"http://www.vibrancy.cn/tags/插入排序/"}]},{"title":"Java8 HashMap原理分析","slug":"Java8-HashMap原理分析","date":"2017-05-03T04:55:16.000Z","updated":"2017-05-04T01:09:51.891Z","comments":true,"path":"2017/05/03/Java8-HashMap原理分析/","link":"","permalink":"http://www.vibrancy.cn/2017/05/03/Java8-HashMap原理分析/","excerpt":"简介特性 它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。 HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 映射中的key是不可变对象，不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了。","text":"简介特性 它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。 HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 映射中的key是不可变对象，不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了。 类继承关系图 内部实现几个重要的属性123456transient Node&lt;K, V&gt;[] table; int threshold;final float loadFactor;transient int size; transient int modCount;static final int TREEIFY_THRESHOLD = 8; table 哈希桶数组 初始化长度length默认为16，长度必须为2的n次方（合数）。 常规设计是把length设计为素数，来减少hash冲突的概率。而HashMap在此是为了在取模和扩容的时候做优化，同时也为了减少冲突。 loadFactor 负载因子，是table中元素数量和table长度的比值。 默认值是0.75 threshold HashMap所能容纳的最大数据量的Node(键值对)个数； 计算公式：threshold = table.length * loadFactor，结合公式可知，threshold是负载因子和数组长度对应下允许的最大元素数目，如果超过这个数目，那么就得重新扩容（resize），扩容后的容量是之前容量的2倍。 如果内存空间大而又对时间效率要求很高，可以降低负载因子Load factor的值。 如果内存空间紧张而对时间效率要求不高，可以增加负载因子loadFactor的值，这个值可以大于1。 size HashMap中实际存在的键值对数量； 注意与table.length、threshold的区别。 modCount 记录HashMap内部结构发生变化的次数； 用于迭代的快速失败。 TREEIFY_THRESHOLD 链表转红黑树的长度阈值。 存储结构从结构实现来讲，HashMap是数组+链表+红黑树来实现的。 从源码可知，HashMap类中有一个非常重要的字段，就是Node[] table，即上图中的哈希桶数组table，是一个Node类型的数组。123456static class Node&lt;K, V&gt; implements Map.Entry&lt;K, V&gt; &#123; final int hash; //用来定位数组索引的位置 final K key; V value; Node&lt;K, V&gt; next;&#125; Node是HashMap的一个内部类，实现了Map.Entry接口，本质是就是一个映射(键值对)，上图中的每个黑色圆点就是一个Node对象。 HashMap就是使用哈希表来存储的。哈希表为解决冲突，可以采用开放地址法和链地址法等来解决问题，Java中HashMap采用了链地址法，链地址法简单来说，就是数组加链表的结合。在每个数组元素上都对应一个链表结构，当数据被Hash后，得到数组下标，把数据放在对应下标元素的链表上。 如果哈希桶数组很大，即使较差的Hash算法也会比较分散；如果哈希桶数组数组很小，即使好的Hash算法也会出现较多碰撞，所以就需要在空间成本和时间成本之间权衡。其实就是根据实际情况实行哈希数组的扩容或收缩，并在此基础上设计好的hash算法减少Hash碰撞。 负载因子和Hash算法设计的再合理，也免不了会出现链表过长的情况，一旦链表过长，则会严重影响HashMap的性能。当链表长度太长（默认超过TREEIFY_THRESHOLD）时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高HashMap的性能，其中会用到红黑树的插入、删除、查找等算法。 核心方法分析根据键值计算哈希桶数组的索引123456789101112131415161718/**** 根据key计算hash值*/static final int hash(Object key) &#123; int h; // h = key.hashCode(); 第一步、取 kek的hashCode值 // h ^ (h &gt;&gt;&gt; 16) 第二步、取hash的高位与hash参与异或运算 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;/*** 根据hash值和数组长度，计算key在table中的索引。* JDK8 中没有该方法，它直接在方法内部计算 hash &amp; (length - 1) 的值*/private static int indexFor(int hash, int length) &#123; return hash &amp; (length - 1);&#125; 不管是增加、删除、查找键值对，定位到哈希桶数组的位置都是很关键的第一步。对于任意给定的对象，只要hashCode相同，那么hash()方法返回的hash值总是相同的。一般情况下，将hash值与数组长度进行取模运算来得到数组索引，但是取模运算的消耗还是比较大的。在HashMap中，通过indexFor()方法来计算索引。 indexFor()方法非常的巧妙，通过hash &amp; (length-1)得到对象的保存位置。因为HashMap底层数组的长度总是2的n次方，这时hash &amp; (length-1)运算等价于hash对length的取模，&amp;比%具有更高的效率。 画图说明hash()和indexFor()的运算过程: put方法 put()流程 源码如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K, V&gt;[] tab; Node&lt;K, V&gt; p; int n, i; //table 是否为空 或者 长度为0 if ((tab = table) == null || (n = tab.length) == 0) &#123; // resize 重新扩容 n = (tab = resize()).length; &#125; //如果当前table索引上的值为空 if ((p = tab[i = hash &amp; (n - 1)]) == null) //直接将值插入 tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K, V&gt; e; K k; // 如果 key 并且 hash 相同 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p;//直接覆盖value else if (p instanceof TreeNode) //如果是红黑树，则直接在树中插入键值对 e = ((TreeNode&lt;K, V&gt;) p).putTreeVal(this, tab, hash, key, value); else &#123; //开始循环，遍历链表 for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; //到了链表末尾 p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st //链表长度大于8转换为红黑树进行处理 treeifyBin(tab, hash); break; &#125; // 如果 key 并且 hash 相同 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) //直接覆盖value break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) //超过最大容量 就扩容 resize(); afterNodeInsertion(evict); return null;&#125; 扩容机制扩容(resize)就是重新计算容量，向HashMap对象里不停的添加元素，而HashMap对象内部的数组无法装载更多的元素时，对象就需要扩大数组的长度，以便能装入更多的元素。 下面举个例子说明下扩容过程： 123456789public static void main(String[] args) throws NoSuchFieldException, IllegalAccessException &#123; HashMap&lt;Integer, String&gt; map = new HashMap&lt;&gt;(2); printInfo(map, &quot;初始化HashMap的信息为：&quot;); int[] values = &#123;3, 7, 5, 9&#125;; for (int i = 0; i &lt; values.length; i++) &#123; map.put(values[i], &quot;v&quot;); printInfo(map, String.format(&quot;添加第%d个元素[%s=%s]后的info：&quot;, i + 1, values[i], &quot;v&quot;)); &#125;&#125; 运行结果如下图所示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657初始化HashMap的信息为：size: 0tableLength: 0loadFactor: 0.75threshold: 2modCount: 0table: null添加第1个元素[3=v]后的info：size: 1tableLength: 2loadFactor: 0.75threshold: 1modCount: 1table: 索引 | 元素 0 | null 1 | [3=v]添加第2个元素[7=v]扩容后的info：size: 2tableLength: 4loadFactor: 0.75threshold: 3modCount: 2table: 索引 | 元素 0 | null 1 | null 2 | null 3 | [3=v] --&gt; [7=v]添加第3个元素[5=v]后的info：size: 3tableLength: 4loadFactor: 0.75threshold: 3modCount: 3table: 索引 | 元素 0 | null 1 | [5=v] 2 | null 3 | [3=v] --&gt; [7=v]添加第4个元素[9=v]扩容后的info：size: 4tableLength: 8loadFactor: 0.75threshold: 6modCount: 4table: 索引 | 元素 0 | null 1 | [9=v] 2 | null 3 | [3=v] 4 | null 5 | [5=v] 6 | null 7 | [7=v] 经过观测可发现，HashMap的table数组长度使用的是2次幂的扩展（长度扩展为原来2倍），数组扩展后，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置，扩展后对于元素新位置的判断对应的源码为：123456789101112131415161718192021222324252627282930HashMap.resize():Node&lt;K,V&gt; loHead = null, loTail = null;Node&lt;K,V&gt; hiHead = null, hiTail = null;Node&lt;K,V&gt; next;do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125;&#125; while ((e = next) != null);if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead;&#125;if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead;&#125; 接下来以添加第4个元素之后进行扩容的过程分析一下上面代码的原理 以上4个元素的hash值分别为： | key | hash | | :—: | :—: | | 3 | 3 | | 7 | 7 | | 5 | 5 | | 9 | 9 | 当把第4个元素[9=v]添加进map之后，未扩容（未执行resize()）前的table为： 12345table: 索引 | 元素 0 | null 1 | [5=v] --&gt; [9=v] 2 | null 3 | [3=v] --&gt; [7=v] 这时由于++size &gt; threashold ==&gt; 4&gt;3 ，所以需要执行resize()方法 该过程为新建一个长度为原来2倍的数组，如果判断原来数组上的node是一个链表，那么会遍历链表，判断每个元素的(e.hash &amp; oldCap)的值是否为0，来决定链表中元素的新位置 | key | hash | (e.hash &amp; oldCap) | 是否为0 | 新索引 | | :—: | :—: | :—————: | :—–: | :—-: | | 3 | 3 | 0 | 是 | 3 | | 7 | 7 | 4 | 否 | 3+4 | | 5 | 5 | 4 | 是 | 1+4 | | 9 | 9 | 0 | 否 | 1 | 根据上表的统计可以得出结论，如果e.hash &amp; oldCap为0，则位置索引不变；否则新的索引是原位置索引+oldCap的，那么扩容后的table为： 123456789table: 索引 | 元素 0 | null 1 | [9=v] 2 | null 3 | [3=v] 4 | null 5 | [5=v] 6 | null 7 | [7=v] 该判断是JDK8的一个优化，不需要像JDK7那样重新计算hash，只需要判断元素的hash值与oldCap的与运算结果就好了。这样的设计省去了重新计算hash值的时间，并且能够均匀的把冲突的节点分散到新的table中去。另外，JDK8的HashMap在迁移链表的时候会保持链表元素的顺序不变。 resize()方法的全部代码如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970final Node&lt;K, V&gt;[] resize() &#123; Node&lt;K, V&gt;[] oldTab = table; int oldCapacity = (oldTab == null) ? 0 : oldTab.length; int oldThreshold = threshold; int newCapacity, newThreshold = 0; if (oldCapacity &gt; 0) &#123; if (oldCapacity &gt;= MAXIMUM_CAPACITY) &#123;//扩容前的数组大小如果已经达到最大(2^30)了 threshold = Integer.MAX_VALUE;//修改阈值为int的最大值(2^31-1)，这样以后就不会扩容了 return oldTab; &#125; else if ((newCapacity = oldCapacity &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCapacity &gt;= DEFAULT_INITIAL_CAPACITY) newThreshold = oldThreshold &lt;&lt; 1; // 将容量和阈值在原来的基础上扩大2倍 &#125; else if (oldThreshold &gt; 0) // initial capacity was placed in threshold newCapacity = oldThreshold; else &#123; // zero initial threshold signifies using defaults newCapacity = DEFAULT_INITIAL_CAPACITY; newThreshold = (int) (DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThreshold == 0) &#123; float ft = (float) newCapacity * loadFactor; newThreshold = (newCapacity &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float) MAXIMUM_CAPACITY ? (int) ft : Integer.MAX_VALUE);//修改阈值 &#125; threshold = newThreshold; @SuppressWarnings(&#123;&quot;rawtypes&quot;, &quot;unchecked&quot;&#125;) Node&lt;K, V&gt;[] newTab = (Node&lt;K, V&gt;[]) new Node[newCapacity]; table = newTab; if (oldTab != null) &#123; for (int j = 0; j &lt; oldCapacity; ++j) &#123;//遍历原来的哈希表数组 Node&lt;K, V&gt; current; if ((current = oldTab[j]) != null) &#123; oldTab[j] = null;//清空 if (current.next == null)//如果当前节点只有一个节点 newTab[current.hash &amp; (newCapacity - 1)] = current; else if (current instanceof TreeNode)//如果当前节点是红黑树 ((TreeNode&lt;K, V&gt;) current).split(this, newTab, j, oldCapacity); else &#123; // 当前是链表 ，保留顺序preserve order Node&lt;K, V&gt; loHead = null, loTail = null; Node&lt;K, V&gt; hiHead = null, hiTail = null; Node&lt;K, V&gt; next; do &#123; next = current.next; if ((current.hash &amp; oldCapacity) == 0) &#123; if (loTail == null) loHead = current; else loTail.next = current; loTail = current; &#125; else &#123; if (hiTail == null) hiHead = current; else hiTail.next = current; hiTail = current; &#125; &#125; while ((current = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCapacity] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 线程安全性并发的rehash过程在多线程使用场景中，应该尽量避免使用线程不安全的HashMap，因为在并发的多线程使用场景中使用HashMap可能造成数据丢失。 多线程测试HashMap的代码 1234567891011121314151617181920212223public static void main(String[] args) &#123; HashMap&lt;Integer, String&gt; map = new HashMap&lt;&gt;(2, 0.75f); AtomicInteger counter = new AtomicInteger(0); map.put(5, &quot;C&quot;); Runnable r1 = () -&gt; &#123; map.put(7, &quot;B&quot;); counter.incrementAndGet(); &#125;; Runnable r2 = () -&gt; &#123; map.put(3, &quot;A&quot;); map.put(8, &quot;A&quot;); counter.incrementAndGet(); &#125;; new Thread(r1, &quot;thread1&quot;).start(); new Thread(r2, &quot;thread2&quot;).start(); while (true) &#123; if (counter.get() == 2) &#123; printInfo(map, &quot;&quot;); System.out.println(map.get(7)); break; &#125; &#125;&#125; 通过阻塞thread1的resize()，再让thread2执行，并进行resize()操作之后，最后打印的结果为： 12345678910111213141516size: 4tableLength: 8loadFactor: 0.75threshold: 6modCount: 4table:索引 | 元素 0 | [8=A] 1 | null 2 | null 3 | [3=A] 4 | null 5 | null 6 | null 7 | nullnull 可见table的size为4，表明map经历了4次put过程，而实际上却只有两个元素，其他元素丢失了，那么接下来通过IntellijIdea的多线程断点调试来演示一下元素为什么丢失。 初始化一个调试环境 用debug调试模拟多线程切换的流程 点击debug按钮，这时断点会走到thread1处； 将HashMap.resize(){next=e.next}处打上断点，并设置挂起模式为thread。 接着开始执行thread1，这时thread1线程会停到刚才的断点处，相当于挂起thread1。 切换到thread2，并取消第2步设置的断点，让thread2能够一次性运行结束，并进行resize()过程。 thread2线程执行结束后，唤醒thread1，让thread1继续执行。 最后，通过打印的结果可知，数据丢失了。 分析 通过分析resize()的源码可知，每次是让table指向一个newTab 123456······threshold = newThr;@SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap];table = newTab;······ 接着遍历oldTab，将原有的key-value存到newTab中。 12345678910111213for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123;······if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead;&#125;if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead;&#125; 在上面的第三步，thread1执行到next = e.next这挂起，接着唤醒thread2去执行，thread2把[8=A]放进map之后，也会执行resize()操作，这时会将 table 指向一个新的newTab，那么thread1的newTab将会失去引用，所以之前存储的值也就丢失了。 解决方案因此，在多线程环境中，使用ConcurrentHashMap替换HashMap，或者使用Collections.synchronizedMap将HashMap包装起来。 JDK8和JDK7的HashMap性能对比HashMap中，如果key经过hash算法得出的数组索引位置全部不相同，即Hash算法非常好，那样的话，getKey方法的时间复杂度就是O(1)，如果Hash算法技术的结果碰撞非常多，假如Hash算极其差，所有的Hash算法结果得出的索引位置一样，那样所有的键值对都集中到一个桶中，或者在一个链表中，或者在一个红黑树中，时间复杂度分别为O(n)和O(lgn)。 Hash比较均匀的情况 编写一个Key类 123456789101112131415161718192021222324252627class Key implements Comparable&lt;Key&gt; &#123; private final int value; Key(int value) &#123; this.value = value; &#125; @Override public int compareTo(Key o) &#123; return Integer.compare(this.value, o.value); &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Key key = (Key) o; return value == key.value; &#125; @Override public int hashCode() &#123; return value; &#125;&#125; 这个类复写了equals方法，并且提供了相当好的hashCode函数，任何一个值的hashCode都不会相同。 创建Keys类，用于缓存Key，避免频繁的GC，而影响HashMap实际查找值的时间。 123456789101112131415public class Keys &#123; public static final int MAX_KEY = 10_000_000; private static final Key[] KEYS_CACHE = new Key[MAX_KEY]; static &#123; for (int i = 0; i &lt; MAX_KEY; ++i) &#123; KEYS_CACHE[i] = new Key(i); &#125; &#125; public static Key of(int value) &#123; return KEYS_CACHE[value]; &#125;&#125; 开始我们的试验，测试需要做的仅仅是，创建不同size的HashMap（1、10、100、……、10000000） 1234567891011121314151617181920static void test(int mapSize) &#123; HashMap&lt;Key, Integer&gt; map = new HashMap&lt;Key,Integer&gt;(mapSize); for (int i = 0; i &lt; mapSize; ++i) &#123; map.put(Keys.of(i), i); &#125; long beginTime = System.nanoTime(); //获取纳秒 for (int i = 0; i &lt; mapSize; i++) &#123; map.get(Keys.of(i)); &#125; long endTime = System.nanoTime(); System.out.println(endTime - beginTime); &#125; public static void main(String[] args) &#123; for(int i=10;i&lt;= 1000 0000;i*= 10)&#123; test(i); &#125; &#125; 在测试中会查找不同的值，然后度量花费的时间，为了计算getKey的平均时间，我们遍历所有的get方法，计算总的时间，除以key的数量，计算一个平均值，主要用来比较，绝对值可能会受很多环境因素的影响，结果如下： hash极不均匀的情况 假设我们有一个非常差的Key，它们所有的实例都返回相同的hashCode值。这是使用HashMap最坏的情况。代码修改如下： 123456789class Key implements Comparable&lt;Key&gt; &#123; //... @Override public int hashCode() &#123; return 1; &#125;&#125; 仍然执行main方法，得出的结果如下表所示 从表中结果中可知，随着size的变大，JDK1.7的花费时间是增长的趋势，而JDK1.8是明显的降低趋势，并且呈现对数增长稳定。当一个链表太长的时候，JDK1.8的HashMap会动态的将它替换成一个红黑树，这话的话会将时间复杂度从O(n)降为O(logn)。hash算法均匀和不均匀所花费的时间明显也不相同，这两种情况的相对比较，可以说明一个好的hash算法的重要性。 总结 扩容是一个特别耗性能的操作，所以当程序员在使用HashMap的时候，估算map的大小，初始化的时候给一个大致的数值，避免map进行频繁的扩容。 负载因子是可以修改的，也可以大于1，但是建议不要轻易修改，除非情况非常特殊。 HashMap是线程不安全的，不要在并发的环境中同时操作HashMap，建议使用ConcurrentHashMap。 JDK1.8引入红黑树大程度优化了HashMap的性能。","categories":[{"name":"Java","slug":"Java","permalink":"http://www.vibrancy.cn/categories/Java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://www.vibrancy.cn/tags/java/"},{"name":"HashMap","slug":"HashMap","permalink":"http://www.vibrancy.cn/tags/HashMap/"},{"name":"红黑树","slug":"红黑树","permalink":"http://www.vibrancy.cn/tags/红黑树/"},{"name":"散列表","slug":"散列表","permalink":"http://www.vibrancy.cn/tags/散列表/"}]},{"title":"Android增量更新：服务器篇","slug":"Android增量更新：服务器篇","date":"2017-04-30T13:40:21.000Z","updated":"2017-04-30T23:08:03.147Z","comments":true,"path":"2017/04/30/Android增量更新：服务器篇/","link":"","permalink":"http://www.vibrancy.cn/2017/04/30/Android增量更新：服务器篇/","excerpt":"增量更新的原理其实增量升级的原理很简单，即首先将应用的旧版本Apk与新版本Apk做差分，得到更新的部分的补丁，例如旧版本的APK有5M，新版的有8M，更新的部分则可能只有3M左右(这里需要说明的是，得到的差分包大小并不是简单的相减，因为其实需要包含一些上下文相关的东西——有时候旧版本10M，新版本8M，得到的差分包可能有5M)，使用差分升级的好处显而易见，那么你不需要下载完整的8M文件，只需要下载更新部分就可以，而更新部分可能只有3、4M，可以很大程度上减少流量的损失。 在用户下载了差分包之后，需要在手机端将他们组合起来。可以参考的做法是先将手机端的旧版本软件(多半在/data/下)，复制到SD卡或者cache中，将它们和之前的差分patch进行组合，得到一个新版本的apk应用，如果不出意外的话，这个生成的apk和你之前做差分的apk是一致的。","text":"增量更新的原理其实增量升级的原理很简单，即首先将应用的旧版本Apk与新版本Apk做差分，得到更新的部分的补丁，例如旧版本的APK有5M，新版的有8M，更新的部分则可能只有3M左右(这里需要说明的是，得到的差分包大小并不是简单的相减，因为其实需要包含一些上下文相关的东西——有时候旧版本10M，新版本8M，得到的差分包可能有5M)，使用差分升级的好处显而易见，那么你不需要下载完整的8M文件，只需要下载更新部分就可以，而更新部分可能只有3、4M，可以很大程度上减少流量的损失。 在用户下载了差分包之后，需要在手机端将他们组合起来。可以参考的做法是先将手机端的旧版本软件(多半在/data/下)，复制到SD卡或者cache中，将它们和之前的差分patch进行组合，得到一个新版本的apk应用，如果不出意外的话，这个生成的apk和你之前做差分的apk是一致的。 增量更新实现使用的工具使用开源的二进制比较工具bsdiff 及其依赖的bzip2 下载完后，得到这样的目录结构： 其中bsdiff.c用于在服务端生成差分包，bspatch.c用于在客户端把旧版本apk与服务端生成的差分包进行合成为新版本apk。 实现的步骤 在服务端，生成新旧版本的差分包 在客户端，使用已安装的旧版apk与这个差分包，合成为一个新版apk。 校验新合成的客户端文件是否完成，签名时候和已安装客户端一致，如一致，提示用户安装; 服务端生成差分包简介由于服务器的不同，因此我们需要将以上的c/c++文件，build输出为动态链接库，以供java调用，其中Window环境生成名为libApkPatchLibraryServer.dll文件，Unix/Linux环境生成名为libApkPatchLibraryServer.so，OSX下生成名为libApkPatchLibraryServer.dylib的文件。下面具体讲一下在Windows服务器上怎么生成libApkPatchLibraryServer.dll文件的 生成dll动态链接库 下载开源工具 如果我们要生成dll动态链接库，那我们就不能用刚才下的bsdiff-4.3了（具体原因下面再说），我们需要在这里bsdiff 下载一个针对win32版本的bsdiff文件，下载页面如图所示： 下载得到的文件如图所示： 里面已经包含了bsdiff.cpp，bspatch.cpp及其依赖的bzlip等文件。 改写bsdiff.cpp（生成差分包） 首先，在MyEclispe中声明一个native方法，如图所示： 接着，为了生成的方便，将该类及完整的所在包复制到桌面上，我们利用javac 和javap命令将该native方法生成c/c++中对应的.h头文件： 这时，我们可以看到在桌面上生成了一个com_cwc_smartupdate_util_DiffUtil.h的头文件。打开头文件，如下图所示： 以上代码需要说明一下，首先函数名的格式遵循如下规则：Java_包名_类名_方法名。其中，JNIEXPORT、JNICALL、JNIEnv和jobject都是JNI标准中定义的类型或者宏。 实现JNI方法 这里可以选用c或c++实现，我们这使用的是c++，因为在上文中下载win32版本的bsdiff中的bsdiff.cpp就是用c++实现的，接着我们打开该文件进行改写。（注意：我这以下的所有操作都是基于VS2013，因为涉及到很多c和h文件的引用，如果使用命令行的话，在连接阶段可能出现函数未定义的情况，为了简便本人直接使用了VS2013来生成dll文件）。 首先，利用vs2013新建一个dll项目，将bsdiff4.3-win32-src.zip解压得到的文件导入该项目中，并将bsdiff.cpp重新命名为com_cwc_smartupdate_util_DiffUtil.cpp，另外，这里还需要三个头文件，分别是jni.h、jni_md.h（在两个在ndk里面找，可以在该ndk根文件夹子下全文搜索一下）和刚才生成的com_cwc_smartupdate_util_DiffUtil.h，最终得到的目录结构如下： 接着我们打开com_cwc_smartupdate_util_DiffUtil.cpp文件，进行我们的改写操作，首先，引入头文件： 之后，把typedef long pid_t注释掉，并添加ftello和fseeko两个函数宏定义，如图所示： 接着，我们来实现com_cwc_smartupdate_util_DiffUtil.h中的函数，代码如下： 这段代码意思是将传入的三个字符串（新旧版本的文件路径和生成的差分包路径）分别放进一个char* 类型的指针数组里面，然后调用appDiff函数，生成差分包。那这个appDiff如何实现呢，我们找到该文件下的main方法，将该main方法重命名为appDiff： 接着，把该appDiff函数向下拖动，会看到这段代码，这段代码用于通过打开旧版本文件，将文件的数据读到名为old的内存当中，这段代码改写如下： 因为lseek，open等函数都是Linux里面的，如果在window下使用会出现打开文件失败的情况。改写的文件如下，其中stream是新定义的一个FILE* stream;文件类型指针： 通过这段代码可以将旧版本文件数据读取到u_char* old中。接着，我们改写读取新文件的这段代码，代码如下： 改写成： 到此为止，我们的所有操作就全部完成了，接下来我们生成dll文件，在生成的dll文件中也注意是32位的还是64位的，不然在java调用的时候会出现读取失败的情况。 在服务端调用dll文件 其中，DiffAppServer64就是刚才生成的dll文件，由于DiffAppServer64.dll需要依赖其他的dll文件，我这需要依赖msvcr100d和kernel32，所以在这里也必须将两个文件导入才行。最后，这三个文件放置的位置如图所示： 在tomcat根目录下的bin子目录下新建一个appdiff目录，然后把三个文件放到这里。当这些所有的操作完成之后，就可以在服务端生成差分包了。 运行效果展示apk文件上传页面面 旧版本文件上传 新版本文件上传 当上传新版本后，服务端后台就自动开始生成差分包的工作了 那么又涉及到另外一个问题，由于比较生成差分包在底层进行，并且非常的耗时，大概需要1分多钟，那么如何显示进度，生成差分包的工作进行到哪一步了？这就需要利用jni调用java方法，将底层的信息传到java代码层。 jni调用java方法，回传底层的进度信息 在声明genDiff 这个native方法的的类中java.cwc.smartupdate.util.DiffUtil声明一个静态的方法： 这个方法表示，在c底层进行调用该方法，传递整型state参数，通过对比不同的整型数据，找到底层对应的执行进度。 让我们返回到com_cwc_smartupdate_util_DiffUtil.cpp文件中，找到我们在上文中实现的本地方法对应的函数： 为了方便演示，在这个函数上面定义一个用于发布进度的函数： 这段函数的意思就是通过反射来调用DiffUtil类内的静态方法publishProgress，然后在appDiff函数内部，就可以在关键的地方调用该函数用来发布当前执行任务的状态了，（注意定义的函数需要在该文件头部声明，不然出现找不到函数的错误提示），另外还有一点需要注意，publishProgress需要调用不止一次，因此不可能每次都通过反射来创建DiffUtil的类对象(创建一次就够了)，因此，我们需要将上面的函数整理为以下格式： 服务端的java代码层 由于publishProgress传递的是整型数据，我们可以自定义一些整型常量来映射出对应的任务状态。还可以在该DiffUtil定义一个内部接口，这样其他类实现了该接口，就可以接收到底层返回的进度。 结论到此为止，整个增量更新的服务端是实现就结束了，总结一下： 我们首先在java中定义了一个本地native方法，通过javac和javap命令生成了对应的h头文件； 然后改写bsdiff.cpp文件，实现刚才生成的头文件中的函数，并针对windows服务器改写bsdiff.cpp的函数内部的细节； 最后进行将生成的dll文件放到了tomcat服务器中，利用System.loadLibrary函数加载dll文件，如果缺少依赖dll的话，就添加对应的dll文件； 最后为了实现进度的显示，我们又定义了publishProgress方法，并在底层实现了该函数，在java端的DiffUtil类中定义了一个接口，让其他实现该接口的类可以接收到底层发布的进度。","categories":[{"name":"Android","slug":"Android","permalink":"http://www.vibrancy.cn/categories/Android/"}],"tags":[{"name":"增量更新","slug":"增量更新","permalink":"http://www.vibrancy.cn/tags/增量更新/"},{"name":"android","slug":"android","permalink":"http://www.vibrancy.cn/tags/android/"}]},{"title":"到底什么时候该使用MQ","slug":"到底什么时候该使用MQ","date":"2017-04-29T22:23:39.000Z","updated":"2017-04-30T23:08:29.954Z","comments":true,"path":"2017/04/30/到底什么时候该使用MQ/","link":"","permalink":"http://www.vibrancy.cn/2017/04/30/到底什么时候该使用MQ/","excerpt":"MQ的用途消息总线（Message Queue），后文称MQ，是一种跨进程的通信机制，用于上下游传递消息。 在互联网架构中，MQ是一种非常常见的上下游“逻辑解耦+物理解耦”的消息通信服务。使用了MQ之后，消息发送上游只需要依赖MQ，逻辑上和物理上都不用依赖其他服务。","text":"MQ的用途消息总线（Message Queue），后文称MQ，是一种跨进程的通信机制，用于上下游传递消息。 在互联网架构中，MQ是一种非常常见的上下游“逻辑解耦+物理解耦”的消息通信服务。使用了MQ之后，消息发送上游只需要依赖MQ，逻辑上和物理上都不用依赖其他服务。 不使用MQ的情况MQ作为互联网分层架构中的解耦利器，那为什么不是所有通讯都使用MQ呢？因为，调用与被调用的关系，是无法被MQ取代的。调用方实时依赖执行结果的业务场景，请使用调用，而不是MQ。 MQ的缺陷 系统更复杂，多了一个MQ组件； 消息传递路径更长，延时增加； 消息可靠性和重复性互为矛盾，消息不丢不重难以同时保证； 上游无法知道下游的执行结果，这一点是很致命的。 什么时候使用MQ场景一：数据驱动的任务依赖 什么是任务依赖 互联网公司经常在凌晨进行一些数据统计任务，这些任务之间有一定的依赖关系，比如： task3需要使用task2的输出作为输入； task2需要使用task1的输出作为输入； 这样的话，tast1,task2,task3之间就有任务依赖关系，必须task1先执行，再task2执行，载task3执行。 使用cron人工排执行时间表 task1，0:00执行，经验执行时间为50分钟 task2，1:00执行（为task1预留10分钟buffer），经验执行时间也是50分钟 task3，2:00执行（为task2预留10分钟buffer） 使用cron的缺点 如果有一个任务执行时间超过了预留buffer的时间，将会得到错误的结果，因为后置任务不清楚前置任务是否执行成功，此时要手动重跑任务，还有可能要调整排班表。 总任务的执行时间很长，总是要预留很多buffer，如果前置任务提前完成，后置任务不会提前开始。 如果一个任务被多个任务依赖，这个任务将会称为关键路径，排班表很难体现依赖关系，容易出错。 如果有一个任务的执行时间要调整，将会有多个任务的执行时间要调整 采用MQ解耦方案 task1准时开始，结束后发一个“task1 done”的消息 task2订阅“task1 done”的消息，收到消息后第一时间启动执行，结束后发一个“task2done”的消息。 task3同理 采用MQ的优点 不需要预留buffer，上游任务执行完，下游任务总会在第一时间被执行。 依赖多个任务，被多个任务依赖都很好处理，只需要订阅相关消息即可 有任务执行时间变化，下游任务都不需要调整执行时间。 MQ使用注意 MQ只用来传递上游任务执行完成的消息，并不用于传递真正的输入输出数据。 典型场景二：上游不关心执行结果 58同城的很多下游需要关注“用户发布帖子”这个事件 比如招聘用户发布帖子后，招聘业务要奖励58豆，房产用户发布帖子后，房产业务要送2个置顶，二手用户发布帖子后，二手业务要修改用户统计数据。 采用调用关系解决 帖子发布服务执行完成之后，调用下游招聘业务、房产业务、二手业务，来完成消息的通知，但事实上，这个通知是否正常正确的执行，帖子发布服务根本不关注。 采用调用的缺陷 帖子发布流程的执行时间增加了 下游服务宕机，可能导致帖子发布服务受影响，上下游逻辑+物理依赖严重。 每当增加一个需要知道“帖子发布成功”信息的下游，修改代码的是帖子发布服务，这一点是最恶心的，属于架构设计中典型的依赖倒转，谁用过谁痛谁知道。 采用MQ解耦方案 帖子发布成功后，向MQ发一个消息 哪个下游关注“帖子发布成功”的消息，主动去MQ订阅 采用MQ的优点 上游执行时间短 上下游逻辑+物理解耦，除了与MQ有物理连接，模块之间都不相互依赖 新增一个下游消息关注方，上游不需要修改任何代码。 典型场景三：上游关注执行结果，但执行时间很长 微信支付 跨公网调用微信的接口，执行时间会比较长，但调用方又非常关注执行结果，此时一般怎么玩呢？ 采用“回调网关+MQ”方案来解耦： 调用方直接跨公网调用微信接口 微信返回调用成功，此时并不代表返回成功 微信执行完成后，回调统一网关 网关将返回结果通知MQ 请求方收到结果通知 总结什么时候不使用MQ？上游实时关注执行结果 什么时候使用MQ？ 数据驱动的任务依赖 上游不关心多下游执行结果 异步返回执行时间长","categories":[{"name":"架构师","slug":"架构师","permalink":"http://www.vibrancy.cn/categories/架构师/"}],"tags":[{"name":"架构","slug":"架构","permalink":"http://www.vibrancy.cn/tags/架构/"},{"name":"消息队列","slug":"消息队列","permalink":"http://www.vibrancy.cn/tags/消息队列/"}]},{"title":"Tomcat多实例单应用部署方案","slug":"Tomcat多实例单应用部署方案","date":"2017-04-29T02:20:53.000Z","updated":"2017-04-30T23:10:26.309Z","comments":true,"path":"2017/04/29/Tomcat多实例单应用部署方案/","link":"","permalink":"http://www.vibrancy.cn/2017/04/29/Tomcat多实例单应用部署方案/","excerpt":"Tomcat部署的场景分析单实例单应用如果不要求周期性地维护tomcat版本，一般的做法是把打好的war包丢到webapps目录下，然后执行startup.sh脚本，并且可以在浏览器里访问就行了。","text":"Tomcat部署的场景分析单实例单应用如果不要求周期性地维护tomcat版本，一般的做法是把打好的war包丢到webapps目录下，然后执行startup.sh脚本，并且可以在浏览器里访问就行了。 单实例多应用是把多个应用程序的war包放在同一个tomcat的webapps目录，这样一来，关闭和启动tomcat会影响所有项目。 多实例单应用各个tomcat都运行同一个应用程序，对应地需要修改不同的监听端口，这种方式通常会和apache httpd或者nginx整合使用，做一些负载均衡的处理。 多实例多应用相当于第一种场景的复数形式，除了修改不同的监听端口，没有本质区别。 Windows服务器下多实例单应用设置流程分离目录 刚解压出来的tomcat目录结构 bin：主要存放脚本文件，例如比较常用的windows和linux系统中启动和关闭脚本 conf：主要存放配置文件，其中最重要的两个配置文件是server.xml和web.xml lib：主要存放tomcat运行所依赖的包 logs：主要存放运行时产生的日志文件，例如catalina.{date}.log等 temp：存放tomcat运行时产生的临时文件，例如开启了hibernate缓存的应用程序，会在该目录下生成一些文件 webapps：部署web应用程序的默认目录 work：主要存放由JSP文件生成的servlet（java文件以及最终编译生成的class文件） 将解压出来的tomcat文件拆分出的目录结构如下所示： 1234567891011121314151617181920F:/DevLibs/Tomcat├─applications│ ├─backend # 主要部署后端模块代码│ │ │ shutdown.bat│ │ │ startup.bat│ │ ├─conf│ │ ├─logs│ │ ├─temp│ │ ├─webapps │ │ └─work│ └─officals-website # 主要部署前端代码，如官方网站│ │ shutdown.bat│ │ startup.bat│ ├─conf│ ├─logs│ ├─temp│ ├─webapps│ └─work├─bin└─lib 修改环境变量 环境变量说明 CATALINA_HOME：即指向Tomcat安装路径的系统变量 CATALINA_BASE：即指向活跃配置路径的系统变量 通过设置这两个变量，就可以将tomcat的安装目录和工作目录分离，从而实现tomcat多实例的部署。 环境变量设置 新建变量名：CATALINA_HOME，变量值：F:/DevLibs/Tomcat 不需要增加CATALINA_BASE，该变量在脚本中动态设置。 打开PATH，添加变量值：%CATALINA_HOME%\\lib;%CATALINA_HOME%\\bin 修改server.xml 修改官方网站（officals-website）web应用的server.xml配置（端口号8081）。 将第22行的&lt;Server port=&quot;8005&quot; shutdown=&quot;SHUTDOWN&quot;&gt; 修改为 &lt;Server port=&quot;8015&quot; shutdown=&quot;SHUTDOWN&quot;&gt; 将第69行的&lt;Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot; 修改为 &lt;Connector port=&quot;8081&quot; protocol=&quot;HTTP/1.1&quot; 将第91行的&lt;Connector port=&quot;8009&quot; protocol=&quot;AJP/1.3&quot; redirectPort=&quot;8443&quot; /&gt;修改为&lt;Connector port=&quot;8019&quot; protocol=&quot;AJP/1.3&quot; redirectPort=&quot;8443&quot; /&gt; 将第123行的&lt;Host name=&quot;localhost&quot; appBase=&quot;webapps&quot; unpackWARs=&quot;true&quot; autoDeploy=&quot;true&quot;&gt;修改为&lt;Host name=&quot;localhost&quot; appBase=&quot;webapps&quot; unpackWARs=&quot;true&quot; autoDeploy=&quot;false&quot;&gt; 修改后端（backend）web应用的server.xml配置（端口号8082）。 将第22行的&lt;Server port=&quot;8005&quot; shutdown=&quot;SHUTDOWN&quot;&gt; 修改为 &lt;Server port=&quot;8025&quot; shutdown=&quot;SHUTDOWN&quot;&gt; 将第69行的&lt;Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot; 修改为 &lt;Connector port=&quot;8082&quot; protocol=&quot;HTTP/1.1&quot; 将第91行的&lt;Connector port=&quot;8009&quot; protocol=&quot;AJP/1.3&quot; redirectPort=&quot;8443&quot; /&gt;修改为&lt;Connector port=&quot;8029&quot; protocol=&quot;AJP/1.3&quot; redirectPort=&quot;8443&quot; /&gt; 将第123行的&lt;Host name=&quot;localhost&quot; appBase=&quot;webapps&quot; unpackWARs=&quot;true&quot; autoDeploy=&quot;true&quot;&gt;修改为&lt;Host name=&quot;localhost&quot; appBase=&quot;webapps&quot; unpackWARs=&quot;true&quot; autoDeploy=&quot;false&quot;&gt; 修改启动和停止脚本 将初始tomcat的bin目录下的startup.bat 和shutdown.bat 这两个脚本分别拷贝到backend 和 offical-website 目录下 编辑startup.bat脚本，增加一行语句，用于设置CATALINA_BASE变量。（backend和offical-website需要同时修改） 1234567891011121314setlocalrem Guess CATALINA_HOME if not definedset &quot;CURRENT_DIR=%cd%&quot;## 在这里将CATALINA_BASE 设置为脚本所在目录set &quot;CATALINA_BASE=%cd%&quot; if not &quot;%CATALINA_HOME%&quot; == &quot;&quot; goto gotHomeset &quot;CATALINA_HOME=%CURRENT_DIR%&quot;if exist &quot;%CATALINA_HOME%\\bin\\catalina.bat&quot; goto okHomecd ..set &quot;CATALINA_HOME=%cd%&quot;cd &quot;%CURRENT_DIR%&quot;:gotHome 编辑shutdown.bat脚本，增加一行语句，用于设置CATALINA_BASE变量。（backend和offical-website需要同时修改） 12345678910111213setlocalrem Guess CATALINA_HOME if not definedset &quot;CURRENT_DIR=%cd%&quot;## 在这里将CATALINA_BASE 设置为脚本所在目录set &quot;CATALINA_BASE=%cd%&quot;if not &quot;%CATALINA_HOME%&quot; == &quot;&quot; goto gotHomeset &quot;CATALINA_HOME=%CURRENT_DIR%&quot;if exist &quot;%CATALINA_HOME%\\bin\\catalina.bat&quot; goto okHomecd ..set &quot;CATALINA_HOME=%cd%&quot;cd &quot;%CURRENT_DIR%&quot;:gotHome 启动tomcats 分别在backend和officals-website目录，用Dos执行startup.bat脚本。 在浏览器输入loalhost:8081和loalhost:8082就可以访问tomcat管理页面了。 参考文章 Tomcat多实例单应用部署方案 一个tomcat部署多个应用实例总结","categories":[{"name":"Tomcat","slug":"Tomcat","permalink":"http://www.vibrancy.cn/categories/Tomcat/"}],"tags":[{"name":"tomcat","slug":"tomcat","permalink":"http://www.vibrancy.cn/tags/tomcat/"}]}]}